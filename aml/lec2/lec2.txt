This was a lecture on the introduction to Machine Learning, covering various real-world applications (like self-driving cars, chatbots, captcha), the data annotation process (how users unknowingly label data for companies), and the difference between traditional CS programming versus Machine Learning paradigms (Supervised vs. Unsupervised, Classification vs. Regression).Citation Guide:****: Refers to the lecture audio file "Aml lect 2 part 1.m4a".Lecture TranscriptionTeacher: As-salamu alaykum. To make things easy... after discussing the definition of a machine, what a machine is, then we have discussed about intelligent machines. And here, if we look at some examples, now we have robotic cars where you have obstacle avoidance happening. You can take BYD, Tesla, etc., as examples. Initially, the car was used for traveling for us, right? It created ease that you... it was used in traveling. But now, today's cars have intelligent machines inside that provide you with very good information with the help of too many sensors. Which involves your, let's say, accidents and things like that... they resolve many other problems like that. Even the drivers, their attention is also monitored with the help of sensors, like whether while driving you are not getting sleepy, things like that.Besides that, we have seen, we have chatbots like Sofi. So, with that, you can do any task management work for yourself. Then we saw that we have maps. Then we have speech... speech-to-text kind of a scenario. We discussed it here. For example, when we call call centers, our data is being recorded there. So, that data... transcript... getting that transcript is quite effective for knowledge mining. Then we saw here that we have Facebook and Snapchat. The best example is filters, which we use quite a lot. Behind that, we have a face recognition system where your face is being recognized and then on top of that, whatever advancements they are doing... they look quite pleasing. So, you can say that the world of ML has many dimensions of life... it has a contribution. Mobile phones, Alexa, email.Then you can say that AI has now started taking our jobs as well. For example, earlier there used to be call centers.Student: They still exist.Teacher: They still exist.Student: They exist, sir.Teacher: So, we used to have them, but now we have chatbots, right? So, communication can happen through that. And they are context-aware enough that they can provide you relevant information in real-time.And then in the last class, we visualized an example like this here. This poor thing here is some crops data. So, for example, in the field of crops, you can also do different recognitions with the help of AI... of weeds and all things. And then we took some examples related to captioning. That, for example, what is the information you can extract from an image? You can say that object detection is happening in this. Besides that, you have... that object... like umbrella and other things. Besides that, you have humans... like animals. And then there is some description on top of it. So this is even more context-aware knowledge you have. So there were many such things that we saw with the help of AI that AML has many dimensions in daily life applications.Then we saw in "Introduction to Machine Learning" that this machine learning paradigm... what kind of paradigm is this? So, we initially saw that we have a traditional CS problem in which we have data and program, and it provides you with an output. And when we talk about machine learning, so we have data and output. And based on that, we talk about learning a hypothesis or a function. And that function eventually becomes our program.And a complete... you can say... a machine learning pipeline includes what? That first, based on ML... with the help of data and your, you can say, your output, you create some models, some hypotheses with yourself. And based on that, you get a program. Now, this is the trained program that we are studying in this entire course. The program written here, which is a hypothesis function, which is $f(x)$, what will it be? It will be all those classifiers that we will study in this course. And after this, if we talk, so we have a traditional CS thing. In that, what will happen? We have a program and data. So mostly, this end of yours, the machine learning one, is based on training. That when you have to conduct training, to get a hypothesis, then how will you be doing it based on data and its input. And when you talk about traditional CS, then you have a trained program and your data. And this data will be unseen data for you. Based on that, some assumptions, some predictions will be coming to you.And then we saw what the types of data are and what we can get from that data. These are points from our previous lecture. So I am telling you quite quickly. So, in this, first of all, we said that we have data in the form of images. And inside the image, we first talked about captioning. And now we have a healthcare problem in which you have an X-ray and you have to detect a tumor from it. So this is another problem which you can understand with the help of imagery. Besides this, we have here classification and prediction happening in terms of Yes or No. Either did... Either this image has a tumor or not. So this is our pure classification problem. And when we are talking about a classification problem, it means it belongs to supervised ML.And then we have another problem which we discussed, that if we have prediction in terms of real values. Like the next example we have is price of a house. Now the price of the house is a real value for you. So, real value also comes under... you can say... supervised machine learning, but in that we have the domain of regression. So we have regression problems in which you have to predict continuous values.Then we have speech. So in speech, we also had many discussions about what we can get from speech. Speaker information can be there, we can tell the speaker's gender, tell their intent, tell whether the speaker is speaking in a native language or not. So many things like this. And in speech, emotion recognition can also happen. That for example, you call a call center or receive any call from any company and they want to do your sentiment analysis. So this speech will be useful for you there as well.Then we have text. So here, you can say that in textual information also we can extract a lot... sentiment analysis, like intent of the writer. Besides that, you can say the context of the writer. We can extract many things from this. So these were some domains regarding which we talked that when we are studying ML... so on an abstract level we have categories, one is supervised and one is unsupervised. And the supervised category... in this we have two major domains, that is supervised and unsupervised... you have classification and you have regression domains.Besides this, we have the domain of segmentation, of detection. But that does not directly relate to this. So our focus in ML will be on classification, on regression, and to some extent on segmentation. And when we talk about segmentation, I am referring to unsupervised machine learning.And by the end, we said that basically AI models are black boxes. Because if anyone among you has studied AI, mostly everyone must have. So you know that if you are training a neural network or training a logistic regression model or any other deep learning model. So you know how many neurons are in the first layer or second layer or third layer. But you don't know what kind of information is stored in each neuron. So that is a black box for you. Just that if you have good data, it will train well, its results are good. You know nothing about anything else, that what value does each neuron have. So if you want to open up this black box, then you have a special domain that is Explainable AI. So then you will go towards Explainable AI or XAI. Where you will try to know how relevant each component involved in decision making is, how much information is stored in it, and because of that, how the decision making is happening. So our course will be more relevant to this... towards AML or Machine Learning algorithms. And mostly there will be black boxes.Student: Sir, when in neurons... like our prediction criteria is set. So don't they have weights set inside? Like...Teacher: Weights are set, but we don't know them.Student: No?Teacher: For example, let's say I have to hire someone for a job. Now what do I have to do? I have to do resume ranking. I get CVs, for example, I have 100 CVs. So first of all, I will see what the educational background is. For example, if I want to get a data science job done. So my preference will be that the kid is from CS or Data Science. Apart from this, if there is a student, I will discard them. Then after that, I can apply a second check, that which university did they study from? LUMS, UMT, UCP, UOL. Then after that come his grades etc. Now this is that information which I am telling by looking. I am setting some rules. But what information is a neuron taking from it, is unknown to me. So does it have weight? It is doing decision making. But it is unknown to us. That feature is unknown to us. So you can say that... by looking at which component it has done decision making... when this is unknown to us, we call it a black box model. So mostly in AI/ML, whatever problems we are doing, we are just adding neurons, adding layers, applying models. But we don't have an actual idea of which feature is how important. So this we said that it's a black box thing. And Explainable AI is the right answer to this.Now let's look at some interesting facts as well. That is that we saw how important our machine learning is, where it is being used. And when we talk about machine learning, the most important thing that comes for us is data. Data is very important. Data distribution is very, very important for us. So can you tell me that in a day... like Microsoft, Google, or all these big giants... are we giving them data annotated? Yes or No?Student: We are giving it, sir.Teacher: How are we giving it?Student: Sir, hiddenly.Teacher: This is... I mean, we use it, so we know that when we are talking to ChatGPT, giving prompts... in that we are also giving our answers. This is a direct way in which we know.Student: Sir, like calls get recorded. Our microphone, camera are always... like when we say something, ads show up. So our microphone is on.Teacher: Look, we are talking to a friend about purchasing something and after a while, you see its listing on Facebook... that you can get it this cheap from here. But that is... they are taking data from us. I am talking about providing data by annotating it. That means labeling data and giving it.Student: Sir, captchas.Teacher: This is a very good example he gave. ReCaptcha... for example... Okay. So, "I am not a robot." So, when you have to be assured that you are human. At that time, basically what is happening is... You are a free-of-cost annotator. So here they will ask you, road crossing is here, here are bicycles or whatever... But they are very intelligent. In the labeled data, there is some data for which they know the ground truth. For example, if they give you 4 or 5 elements to mark, so they know about two valid examples. So with that, they also get to know the validity of your labeling. So if you... out of all... you must have seen that out of 4 or 5 instances, if you do even two, it gets accepted many times. The reason is that they have ground truth for only two images... which you tell. So, their authenticity comes to them, and the rest of the work you are doing, you are doing for the company. So ReCaptcha is an example in which data sources... we are working like an annotator. And whether we know it or not, but by the end, this data is then being used for retraining. Some examples in this are quite neat and clean, good ones. But some are very much corner cases. Which we find with great difficulty. So you are annotating very complex data like this and giving it free of cost. So this is one example.Apart from this, we talked about call centers. That here the voice recording... it converts into text-to-speech. And from here... you can say... and after that, after every call, they tell you that if you are happy with this call, press one or press two. So that is kind of a... you are providing sentiment analysis kind of data labeled... you are putting a label that I am happy with this entire conversation. And then... they have experts who filter it further. But initially, they get a label.Okay. Apart from this, what else are we doing? Email categorization. So, you... for example, some good kids define separate tags for this. That these are my university emails coming, these are coming from my friends. So this is another type of labeling that we are providing... in terms of...Student: Sir, university usually does it... special labeling.Teacher: But whatever they do, it becomes enough for them. So you can say that in textual domain, this kind of labeling, this kind of categorization that we are doing, eventually its answers serve as a label, as a ground truth for them.Okay. Apart from this, any other example?Student: Sir, location on mobile phone.Teacher: Absolutely, location too. You can say that your mobile phone is also a very good data source for them, which you have traffic blocks... which are coming dynamically on maps. That is because of your GPS. Satellite imagery is there, plus...Student: One is that on Google Maps... otherwise we can pinpoint that a traffic policeman is standing here or not. Secondly, traffic...Student: I was saying one is map... tracking happens that we can mark whether this road is closed or not. After that, the emails we receive, we can categorize them as spam, not spam maybe.Teacher: So, what you call mark as spam, non-spam... so that is also information going to train their model.Student: Sir, the voice thing you talked about... in that voice is there, then we did annotation. Sir, isn't that quite generic annotation? That are you satisfied or not?Teacher: Google teams will be there... But...Student: Sir, labeled data in captcha... like what is special in that? It is labeled, but...Teacher: It is labeled, but corner cases are provided in it. For example, in one place there is a car... absolutely fine. In one place, it is a bit protruded, a portion is showing. So a human can classify it much better. Or it is in a shadow. Or it is in a dark area. So you can tell. So such complex cases... And in this too, you can say that if surface-level filtering is done, then to filter it further... they have teams. So you are not doing all the work for them. But making it easy.Student: Sir, the ReCaptcha ones also have boxes, like we make boxes in object detection. So don't their own algorithms work in that? Like object detection ones...Teacher: The thing is about confidence. An annotator agreement happens... biasness... We have so many things. In which, the more persons... maybe one image is annotated by many people. So by the end, based on confidence, they use it.Okay. Apart from this, there is one more thing. That is, say, on Facebook too we provide a lot of such conversational... sentiment analysis or conversations... we are telling ourselves. For example, we put up an image...Student: Feeling excited.Teacher: I was also wanting you to look there. Feeling excited. So now, for example, you put up an image, now you told there "feeling excited". Now in "feeling excited", first of all, they will get your facial features... how do you look happy.Student: Or excited.Teacher: After that. After that, it can happen that you have a specific time... the conversations you are having... You can tag all of that... that after happiness, the call you are recording, talking, texting... what is your sentiment in that? So, these kinds of informations, willingly or unwillingly, we are giving all this information to our Facebook. And because of this, the algorithm has become so good that the thing we are thinking is appearing on Facebook. That "I was just thinking about this, I hadn't even talked to anyone." So we have given them so much information from which their model is trained.Then Facebook... I was scrolling so I added some more data from it... A while ago, some time ago... a "10 Year Challenge" or something... hashtag 10 years... something like that had come. And all people getting excited were picking up their childhood photos... then telling that "I was like this 10 years later", "I was like this 5 years later". Basically, inside this, the scenario you have is that... the growth of your features... that can be captured here. Besides that, facial recognition can be more robust. I only mean that facial recognition and facial things... whatever robustness of the model is... If you keep a specialized person, a trained person... a team... for them collecting this data is very difficult. That they first see what my image was 10 years ago, what was it 5 years ago, what was it 3 years ago. Poor fellows, those who didn't have 10 years ago, they started putting 5 years one... that this is the 5-year gap, this is the 6-year one. So you can say that in this way, many such challenges come... basically we are working as data annotators for them. And whether we know it or not, but by the end, this data is being used for retraining. Last one... sorry for that... No makeup filter trend. So, this is a 55-year-old lady. So you can say that... there are many such tasks in our daily life... without knowing... we are working as data annotators for them.Student: Sir, but this data won't have any credibility. Credibility...Teacher: Sir, now if I ask you... if you are putting up a 10-year-old picture of your own will... So let's say in the whole class, you don't put it right. Others will put it right.Student: Sir, but credibility is less...Teacher: No, look... I have searched... many of my friends put it up right. If I look being an evaluator... so many people have put it up right. So when you set a trend... so when that trend is followed by everyone, celebrities follow it... so eventually we also follow it in the right way.Student: Outliers...Teacher: Even if there are outliers, so what?Student: Sir, when we upload such things... metadata also goes with it. So in that creation date, modification date... help is available.Teacher: Exactly. But see, when we create a new copy... does metadata come in that? We move data from one place to another, shift it, copy it, does the change come?Student: In copy, mostly I think... minor things get overwritten but mostly it maintains.Teacher: But the metadata is not that reliable. Now someone scanned a 10-year-old photo. Now the scan will give today's date. When was that from?Student: Actually, that stuff is needed where you have to verify... like how will you look 10 years later... so for that...Teacher: Look, in this... listen to me. There are many other challenges like this. But their objective was data annotation. And any problem whose robustness is being worked upon. For example, you say that you collected data in a region... but there are many regions that are not accessible to you. Then you create such a trend and get data from them. We have provided so much data, that's why our models are working so well.Okay, I won't stay on this image for long. I will switch next time. So now... we have previously discussed about intro, data sources, data collection, data dimensions... we have talked a bit about everything and we got some idea of how we are providing data or they already have it. Now let's see how we can process data.So the first thing is that the data you have in supervised learning... if it's a classification paradigm... so we will have a feature space. Features are just attributes based on which you represent any single person or entity. Now if here... I tell you the dataset... I have a four-dimensional dataset. In which you are taking height, weight, and BP information of patients. And from that, you are predicting... you want to predict whether they have heart disease or not. So whether heart disease is present or not... we have a label space. In label space, we have decision making coming as a Yes or No. And if we, for example, took 7 instances here... in which height, weight, and BP information is taken... and then we have labels of it. So all these things... if we teach this to our function... so because of this, it will be learning some information, some hypothesis. So as a result, this will become our training data. And we keep some instances like this against which, for example, there is no label, so you have to predict. So this prediction one will become your test data. So whenever data D comes to you... you have a training set and a testing set. And if data is labeled... and in labels you have categorical values... and more specifically binary values, Yes or No. In that case, it's a classification problem, binary classification. And now we... let's see... this was our classification paradigm.Besides this, what can we have? Besides this, we can have regression paradigm. In regression paradigm also if you see... again we have four-dimensional features. In this, the information you have... you can say is numeric information. And what you are predicting... those are also real values... numeric values. And in this case, we are predicting cholesterol level. So in that case, we have a regression problem.Student: Sir, aren't regression values continuous?Teacher: They are continuous values. Continuous... but they seem discrete to you... there isn't a gap like that. So even if the data was for house price prediction... there would be values in points too... gap would be less. So still these are continuous. You don't have a specific range... but there are some values in it.So, now we... let's see... these same categories... we will look at them in detail. First of all, we will be looking at what can come in feature space. And after feature space, we will be looking at what can come in label space. Based on that, our paradigm will be getting selected... that either we are going for classification or regression.Rule versus Learning... you have that paradigm which we discussed in Class 1. That like we have emails and we say... like either it's spam or not. So in that case, we knew that our probability distribution of spam emails... so this is not same. To make this distribution, many sources are involved from which we have... you can say... data coming. For example, you gave an email address somewhere. From there spam... you are getting emails. Besides that, you bought something. Or you can say besides this, you have some other... like this kind of problem came. So based on some specific keywords... you cannot describe this distribution based on keywords... that if these keywords appear, add it to spam. So for this, what will I need? For this, I will need labeled data. Which as I told you a while ago, you will be providing yourself too. You have email, along with it is the label "Spam". Now what are you doing? Based on that data, you are learning a mapping. That if this kind of data is there, then this is spam for you. Now with this, it will happen that every new category that comes... when you label it with spam... so in the end what will happen is that... you can change the mapping. Retrain. By adding new labels in it, adding new data, modifying it. So in that way... you can say... now it can also happen that for example, you made rules of English. So you got a spam mail in Urdu. So language change can also happen. When language changes, then obviously your rule-based agent won't work. So you will need learning. So in that case, if you have a model prepared... a mapping is prepared... you can add new mapping in it and make it learn new rules. So learning and rules... we have this slight difference.

...probability distribution for children and adults. If we are talking about children, we need to look at certain features that we can consider for both cases. For example, if I bring in the feature of 'weight and height,' we don't know the exact probability distribution because we don't know the entire population of every child and every adult. But we can approximate it.This approximation is happening from the data. For example, if you have data for 50 adults and 50 children, it will create a probability distribution where the lower end might indicate the distribution for children and the other for adults. But again, we say that the larger the dataset, the more instances you have, the more valid the probability distribution becomes. That’s why we say we should have more and more data.For example, if there are 100 cases, the probability distribution will look one way. If I talk about 100,000 instances, the probability distribution might look different; the separation might look different. So, based on these probability distributions, we try to achieve this so we can build a mapping between them. For example, if we get more cases, maybe this area gets covered, which is currently uncovered. Similarly, more cases could appear here. So with new data, we try to approximate this probability distribution as best as possible.Because if we already knew the entire real-world distribution, we would just look at the probability for 'Y given X,' check the label and a few things against it, and get the answer. But in reality, it doesn't work like that. The probability distribution is unknown to us, which we approximate later.In this, we then looked at the representation. We said that in formalizing this set, within the feature space, we have an X vector and we have a Y. So all your data comes from X and Y, 'D' means our complete dataset, and X is our d-dimensional feature space. In the last example we saw, our feature vector had a dimension of 4, and the label space had a dimension of 1. However, it's possible that the label space could also contain a vector. But initially, what we have studied so far is that we have 'Xi' as a vector, where 'i' refers to each new instance we are talking about.Here, we try to approximate the probability. And we have 'h' as a hypothesis function. This is the function that will generate predictions for us. Or you could say the crux of this entire course will be this 'h' function. And what is this 'h' function? It belongs to capital H. And what will capital H contain? All the classifiers we will have studied here. For example, Logistic Regression. For example, Naive Bayes, SVM, Neural Networks. So you can say that in this hypothesis space, you can assume all classifiers exist.Initially, we call this an intelligent program, which is trained with data, becomes intelligent, and by the end... how does prediction happen? You provide the feature vector as input to 'h.' Now, this 'h'—for us—is a model. That model could be anything: a neural network, SVM, Naive Bayes, anything. But as a result, it has to produce Y. Y will be its label. Now, how good that label is or isn't becomes a question. And there are different methods for its evaluation.But initially, our objective is that when any data comes to us, first of all, we bring it into valid features, populate those features, and populate them so well that they are a true representation. Initially, when face recognition models came to mobile phones, what happened was that Nokia introduced face recognition, and it was mostly trained on people from the USA. Now, their complexion and facial structure were seen very well by that model. But as soon as it launched in South Africa, Pakistan, etc., it failed.It failed because your model had never seen a black face, or faces like ours—it hadn't seen them before. So what was it going to predict from them? The model wasn't trained that way. That is why data distribution is important to us; we don't have all the data distribution available, but we have to approximate it. So what you can do is introduce some Chinese faces, some faces of people like us, some Africans. We have to take the probability distribution near to perfection so that when the model is being trained or a hypothesis is being developed, it happens on very realistic grounds.Then comes the final thing: evaluation. Evaluation involves training first, then testing, and then evaluation. In evaluation, you have a comparison between the output and your correct answer. So, where we wrote h(x) = y (meaning it produces y for us), we now have to see if that produced 'y' is, in a true sense, the same 'y' that was required or not. So we compare h with y.The formal definition of Machine Learning is enough: 'A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.'The simple answer to this is: if your model improves with experience, this is Machine Learning. For example, let me give you an example of resume ranking. You can make a rule that if you get a resume from certain universities, you offer those candidates a job. This might be valid for the case where the student is from a good university, has a good grade, and performs well. What if a student from an unknown university comes with good grades? How will they perform? What if a student from a good university comes with bad grades? How will they perform? You cannot select based on rules.You can understand it like this: For example, if I tell Rafay to sit with me and we conduct an interview. Now, their job is not to speak—it’s a difficult task, but we will try. Their job is just to observe my questions, observe the student's answers, and note my final verdict. Now, would it be a good verdict if 20 students came for an interview in a day, I rejected 19, and picked one candidate? What will Rafay learn from this?(Student): If the interview went well...(Teacher): No...(Student): It's skewed...(Teacher): It's skewed. It means your data is skewed. You only provided negative examples. You didn't have positive examples. It's imbalanced. So Rafay's learning will be that whatever comes in, I just have to reject it. That's it. We'll seat him the next day; even if 100 kids come, maybe one lucky one gets in, otherwise no one can.This is because if Rafay sits with me for two or three days while we conduct interviews, learning my questions, learning their answers, and by the end seeing the final verdict of selection or rejection—these become hypotheses for us that after two or three days, he could conduct the interview in my place. Whether we should keep this kid or not. Again, he will get some mapping from the data: these were the questions, this was the answering, how relevant the answer was, and the decision-making (true or false). Based on that experience—and maybe if they do this for a day or two, select the wrong people, get called into the office, drink some tea [get reprimanded], maybe reinforcement learning happens, they learn a bit more—they start making better decisions. Models do the same thing. The vaster the data and decision-making it encounters, we observe them.Algorithm that improves on some task with experience—this is the formal definition of Machine Learning.Before this, we looked at the setup and what things are involved. Now let's look at the feature space. All of you are 8th-semester students, some might not be, but that's okay, everyone is a good student here. What kind of features can be in a feature space? Simply put, what can be written in the columns?(Student): Numerical data...(Teacher): My friends who are talking besides me... Yes, [Student Name]?(Student): Sir, this is my first class, I need to coordinate first.(Teacher): No problem. What can come in features or columns?(Student): Sir, whatever entity we have, the information we want to store about it can come.(Teacher): In what format?(Student): Ordinal data...(Teacher): What is ordinal?(Student): Like male/female...(Teacher): No...(Student): Categories...(Teacher): Look, listen to me. She is saying it correctly, but we need to understand it in detail first. The spaces we looked at first—we saw images. First, it is important to know what kind of data can come and what kind of decision-making can happen against it. So first, understanding this is important in this context. So, for example, if you have images. What kind of data comes in images?(Student): Pixel values... RGB...(Teacher): RGB is about the channels or lists inside it. But in what format is the actual data?(Student): Binary... Numeric...(Teacher): One option is Numeric. It will have numbers. Listen... you will get all kinds of data eventually, just listen for now so you know how many formats of data we cater to and how to utilize them. Feature space involves the feature that acts as input to your hypothesis. The output of h(x) is the label space. So we have to see what we can predict in the label space and hypothesis space.First of all, in the feature space regarding images, numeric data can come. Numeric data depends on the number of bits. For example, if it's an 8-bit image, the maximum value can be $2^8 - 1$, which is 255, starting from 0. But you won't get a value like 1.5. This means these are numeric discrete values.(Student): Zeros and ones...(Teacher): Zeros and ones... As [Student Name] or Ali was saying, zeros and ones. So, it's possible it could be a 1-bit image, a binary image. In that, the value will be in the form of 0 or 1. So, the number of bits determines the data in your images. One domain is this.(Student): Where are binary images used?(Teacher): Many places. Have you seen a mask in a dataset? For example, to localize an ROI (Region of Interest), you make those pixels 1 and the rest 0, effectively eliminating them. So we can create a mask like this to overlay and show our object. So, binary can come, to some bits, or more number of bits. This data comes in the form of images. This is also one dimension in the feature space—numeric data which we use. Apart from this...Apart from this, the data we have... one type is Numeric type, where I mentioned numbers. One type is Categorical. In Categorical, we talk about Nominal. Nominal, you can say, is just categories. Just like Binary is also Nominal, but it's a special type (Yes/No). If we talk about Yes, No, and Neutral, it becomes Nominal. In this, data can come in the format of Yes/No, True/False, Success/Failure. For example, you have a data column labeled 'Is Male.' If 'Is Male' has 0 written, it means female. If 'No,' it means 0, so female. So you can say this kind of data can also be seen in the feature space. Any feature's presence or absence measurement, you represent with binary.Then we have Nominal data. Nominal data means there is no relation of that sort. For example, colors: Red, Blue, Green. For example, Blood Type: A, A+, B-, etc. These blood types don't have a relation among themselves in that way. These fall under Nominal. Here you see one thing: 'Operations.' The word 'Operations' I wrote here is in the context of pre-processing. For example, how can you use these features? If I say data is missing. If that feature is categorical and nominal, how will you fill the missing values?(Student): Mode...(Teacher): Based on frequency. Mode. You cannot take their mean. You cannot average them out. Furthermore, you cannot sort this data. You cannot align it, so the median cannot be calculated. The mean cannot be calculated. It means you are checking it with frequency. And another thing this operation reveals is that you can show the dominance of a feature within the prediction. For example, in the Titanic dataset on Kaggle (survival prediction), if a feature is 'Is Male,' you can also see how many males survived that incident. So some operations are based on decision-making (how your model sees it in context) and some on how we can pre-process it. So Categorical and Nominal is one category.After this, we have Ordinal. Zainab, the student next to you, was right. Ordinal features have some rank or relation existing within them. For example, user experience. If you fill out a form, it shows 3-4 levels from unsatisfied to satisfied. These levels are related to each other. You cannot use them standalone. And let me give you another hint: when you process Nominal data, you treat each feature as a separate entity, as a standalone feature (presence or absence of something). But when we talk about Ordinal, they are related. Like for a specific job, the education status must be MS. Here, relating this MS to other education is important for your hypothesis; that minimum MS is required. It means you cannot call it a standalone feature like 'Is MS', 'Is BS', 'Is PhD'. You encode it in a very specific way called 'Encoding.'When we talk about Nominal features, you can simply do One-Hot Encoding or One-Code Encoding. For example, you include this feature as a standalone presence/absence. But when we talk about Ordinal, presence and absence matter, but the rank matters. So in this, numbers can vary. If this feature value is 2, 3, or 1, its impact must be learned by our hypothesis. That is important for us. And again, substitution here can be based on the median, because the data can be sorted; it is related data. And there is no hard and fast rule that for every problem we blindly apply this; dependencies exist from problem to problem. Missing values is a separate topic; you might even have to include other data to fill a specific feature.So, we have seen in Feature Space: Nominal values, Binary values. Binary, after that Ordinal values. Numeric. Here, values can be numeric. And the example I gave was images. Or a feature could be 'how many times you visited the dentist.' That is a discrete value. However, along with this, you also get values of a numeric continuous nature. For example, height.If data is numeric continuous, how does it affect your hypothesis? Between 0 and 1, how many values are there? Infinite. And if we talk about 0 to 10, looking at points, there are huge values. So basically, even with continuous values, we prefer to discretize them to some extent. We make bins. For example, this value lies in this interval, so we keep this value, even if there is some error.Values usually come from a continuous environment (like an image signal), which is later discretized. We perform quantization to see what level we want to discretize to. You might have heard of 8-bit images. Recently, there was something about 1 billion pixels or bits of information. When we go to that many pixels, it means we are trying to keep the continuous signal as continuous as possible within the continuous range. One value is from 0 to 100,000; you make 5 bins and keep 5 values. Those 5 or 8 values can be represented in 3 or 4 bits. Have you ever seen small images, 224x224? When you look at them, you feel no structure is visible. If there are colors, they are very dull. So, from continuous space to discretized space... it implies how much we want to discretize it. We stop at a specific moment; we don't let it remain very continuous.So in Feature Space, we have seen: Nominal values, Binary values, Ordinal values, Numeric (Discrete), and Numeric (Continuous).After this, we also get information relevant to intervals. A feature like... [Student Name]? After this, we get interval-relevant information, like temperature. This data is seen in time-series problems. For example, weather prediction, or abnormality in weather like smog. In smog or particles, you would definitely be looking at a feature like temperature. In this, there are differences that are meaningful. But you have to assume some values, like 0 degrees doesn't mean 3 degrees, or something like that.Apart from this, other interval data comes with Time Stamps. Like Bitcoin. If anyone has seen Bitcoin data, it has a time stamp: highs, lows, features. Those are time stamps. This text... text is never understood by your model first of all. Before this, all the vectors we saw are dense. Dense vector means...Now we have seen feature space in the form of nominal, binary, ordinal, numeric, numeric discrete, and numeric continuous values.Then, there is also interval-relevant information. For example, temperature data used in time series problems like weather prediction or smog detection. Temperature differences are meaningful here.Another type of interval data involves timestamps, such as in Bitcoin data which includes highs, lows, and features with timestamps.Regarding text, models do not understand text initially. Before this, all the vectors we discussed were dense. A dense vector means...... that the feature vector, for example, a 4-dimensional vector, can also be a 2D or 3D array. And these numerical values are discrete.This covers the values we have seen, the operations on them, and we can use mean, median, mode, and other arithmetic operations on numeric data. There are many reasons for using these, such as data normalization. Different data ranges can be problematic for hypotheses, so normalization is important. We can check variability using variance and find central tendency using the mean.Data can also be in the form of text. But text is never understood by the model directly. Before this, all vectors were dense. Dense vectors mean...
So your four cells are fully utilized with valid values. Now, like your image. Now, your image is a very large image. It is three-dimensional, plus it has many pixels. But still, it is considered as a dense vector representation. It means that every pixel brings some meaningful information to the image.

But if we talk about text. So text, if you are given, for example, some books are given, like all the books of your courses are given. So how many unique words will be there? There will be a lot. Now those unique words, you do not give those unique words as is to your hypothesis. Rather, you give it in the form of a representation. And the name of that special representation is embedding. That you are giving your text in embedding form.

Now embedding means that for example, you picked a topic. Now whatever words are coming in that topic, you first made a dictionary in which all those words are coming. And you named that dictionary Bag of Words. That whatever words are there, they will be used in my one dictionary. Now if you have to highlight a specific topic from this, then you will see which word from this topic has appeared. So such a large vector that you have, here some information is also given, for example, here if there are 6 lakh word forms, so if there is a vector of 6 lakhs representing the information of any one of your courses. So how many minimum words can it hit? 100? 1,000? 10,000? But the rest of the words for it will be what? They will be useless, right? There will be zero in all of them.

So this kind of representation, we call it as a sparse representation. So sparse means, sparsity means that there are many entities in it that are useless. But when you have to give words to any of your hypothesis, you will tell it everything that all these words are there, these are the useful ones. Based on that, it will predict something for you from this. Like for example, if you talk about bubble sort or you talk about AI ML, if you have all these keywords, then your hypothesis will tell that you are doing some work relevant to a CS degree. But if you have English or mathematics, the hypothesis will be telling you that your degree program might be linguistics or something. But you have to feed all those words to it. And here a word is being used that is term frequency. So term frequency is that what is the occurrence of terms you have. For example, how many times a word has appeared in any one text. So text as it is does not go to the model. Text goes to the model in the form of embedding. And one bad representation in that is the sparse vector.

And sparse vector, if you want to make something better from it, for example, if you want to make something better from this, then here you have the concept of word embedding. So what are you doing in word embedding? You have some kind of, you can say, hypothesis or some trained models that are taking those words and giving you a dense vector of a fixed size. For example, on one side there is one vector with 6 lakh values and on the other side you have a vector of 512 values.

I am talking about your web search. In web search, behind that we have a BERT model or you can say the models we have initially. So we have so much vocabulary on which they are trained, we study this in detail in the NLP course. That after being trained on billions of words, after that it gives you information of a 512 values, 512 cells, which if you ever visualize, then based on category, it distinguishes many things for you beforehand. Like persons, animals, drinks, chairs, all these kinds of things, if you give it as a sparse vector input, the meaningful representation it will make for you, if you ever visualize it, then they will differ a lot from each other. So you can say that in this case the operations we have come up, cosine similarity comes up, Euclidean distance comes up, dot product comes up. So these are all those things by which you can check every new incoming instance that either what related talk is happening here. Like here when we teach NLP, there we see like comedy books, horror books, so when you give words like that, it automatically makes you lie in that category. So inside the feature space, you can also take sparse vectors, dense also come in the form of embeddings.

So this is that One Hot Encoding vector I was talking about initially. If you have, if you have data say that is nominal, and inside nominal I am talking about where there is no relation. Like for example, shirt size is... no, shirt size will be ordinal or nominal? It will be ordinal because some relation exists between them. So tell me an example of nominal. Gender can come, shirt types can come, formal, casual, you can talk like that. Movies. Formal, casual, say some other X. So you can say that if you have these three different categories. So if you have to make a One Hot Encoded vector of this, then you can make it this way. You place any one cell as a one and make the rest zero. Or you can also use its alternative. That is 1 1 0. So the one that is zero will be meaningful information for you.

This encoding of yours, this will work in nominal when features are not related to each other. But when we will be talking about ordinal. Listen to this, it might be useful for some assignment of yours. So when we will be talking about ordinal, so in ordinal we prefer mapping. So the one that is ordinal, inside that we have one vector only. And for example if it is small, medium and large, so we call it as a 0, 1, and 2. So in this, you explicitly tell, you want to make your hypothesis learn this that look 0 and 1 and 2, they have some meaning. In nominal, your hypothesis will be confused because then it will try to build some kind of relation between them which will not be there. Nature. So, the three feature spaces we had were these.

Let's come to label space. So what comes in label space? Sir, like the shirt size you wrote ordinal. Basically, in ordinal it happens that a relation exists. Like you say your income. Low, medium, high. Relation exists. Low will be some value. We don't know but a relation exists inside it. Medium, high. So when the model has to get a feature of an income, for example, then it will want that there are values like 0, 1, 2, 3 in it. So that based on that it can do some decision making. But if here like shirt type, or some other feature comes like this, which is nominal, then there will be no relation between them. So sir, the one-hot encoding we do, we use it for nominal, not for ordinal? Not for ordinal. So here like color, so this color red, blue, green or your marital status, you cannot do this 1, 2, 3. If you do 1, 2, 3, your model will be confused, your hypothesis, that maybe some relation exists between them. So it is not like that, due to this reason you do not do it through One Hot Encoding.

[Student Question: When I am making an ordinal encoder, I will know the map book, right? So if I do one-hot encoding, when the model is trained, won't it learn through data that the distribution was such...]

Feature will become unimportant. The essence of a feature, right? That will finish.

[Student Question: Even with one-hot encoding, the new feature that will be made, won't it keep that concept?]

It basically, it will consider it as a standalone feature. Because the rest will all be zero, one will be one. So it will see this that for example it's high income. But it won't be able to relate it with low and medium. Relation won't be built. So that feature might not remain that important.

Okay, just one last thing, let's see this. Okay. Now what comes in our label space?

[Student Question: Sir, I missed... did you say ordinal or nominal?]

So, many things combine to make what I am telling you, many different fields have data and their manipulation. If you are finding it this easy, then hold on a bit.

I have told you in feature space what can exist from which domain. Text, video, time-series, in every way. Whatever feature can come here, you will get it. Let's look at label space, what comes in label space.

In label space, first of all you can say if we talk about classification, then binary. That means yes or no. It means if your last feature has two values coming, then it is binary. And in this you can say that value can be 0 1, can be -1 1, or you can have categorical value. About tumor. Benign, malignant, like this. And if these values go beyond two. Then this will come in multi-class classification. And example of multi-class classification is that if I add another positive negative sentiment, neutral and all the things. So this binary class problem of ours, this will become multi-class, more values will come in it.

So in this, categorical values only will come to you. Or binary can come. Or, look at this. For example, you have to recognize a word. It's speech data. You have the text. So now out of that, the text you have pronounced there, now how much is the vocabulary? In our case, for example, 6 lakh words. So it means that the maximum vector that can be made in this, what can it be made of? It can be of 6 lakhs. And every single instance you have will represent a separate class and that will be a word. So, the range of this is up to here, you can add this many classes in it too. And then the regression problem we have, in that you have continuous values, those you can observe.

[Student Question: Can label space have multi-label?]

Very good question. You will benefit a lot from this. They are saying can there be multi-labels? Like, it means many to many. Before we were looking at many to one. Many means many features and only one output. Can there be a many to many case? It can happen. In case of speech it can happen. For example, inside a single text there can be multiple sentiments. Or if I tell you more about this, inside a single image there can be multiple objects as a subset. So this category absolutely exists.

So in the next class I will try that if you come at 4 o'clock, and this is a little bit... son, I will also come. So it is that in the first 10 minutes, completing the remaining portion of the hypothesis, we will move to Naive Bayes.

Cross-ValidationCross-validation is a technique used to evaluate the generalization ability of a model more robustly, especially when the amount of available data is limited.It helps reduce variability in model evaluation by averaging performance across multiple subsets of the data.Left Panel:100 samples$k = 4$fold 1 = 25Iteration 1 :Train :fold 2, fold 3, fold 4.Validate :fold 1Right Panel:TestingAccuracy MeasureAverage Loss(L1+L2 +L3 +L4)/4Finally, train on the whole data once, before shipping out$k$ –fold Cross-Validation AlgorithmInput:Dataset $D$ with $n$ samples.Number of folds $k$ (typically 5 or 10).A machine learning model (e.g., linear regression, decision tree).Performance metric $M$ (e.g., accuracy, mean squared error).(Visual Note: $n = 12$, $k = 3$, Data: 1 through 12)Steps:Step 1: Shuffle the dataset $D$ randomly (optional but recommended to ensure the data is randomly distributed across folds).Step 2: Split the dataset $D$ into $k$ equal-sized folds $D_1, D_2, ..., D_k$. Each fold contains approximately $\frac{n}{k}$ samples.Step 3: For each fold $i$ (where $i = 1, 2, ..., k$):Training: Use the remaining $k-1$ folds as the training set, i.e., combine all folds except $D_i$$$\text{Training Set} = D_1 \cup D_2 \cup \dots \cup D_{i-1} \cup D_{i+1} \cup \dots \cup D_k$$Validation: Use fold $D_i$ as the validation set.$$\text{Validation Set} = D_i$$Train the model using the training set.Evaluate the model on the validation set using the performance metric $M$. Store the performance score.Step 4: After completing the $k$ iterations (i.e., training and validation on each fold), compute the average performance score over all $k$ validation sets and the variance of model performance:$$\text{Average Performance} = M = \frac{1}{k} \sum_{i=1}^{k} M(\text{model on fold } D_i)$$$$\text{Variance of Performance} = \frac{1}{k} \sum_{i=1}^{k} (M(\text{model on fold } D_i) - M)^2$$Output:The average performance of the model over all $k$ folds.Optionally, you can also look at the variance of the performance scores to assess the model's stability across different subsets of data.

Bayes Theorem Derivation28 January 2026 03:081. Conditional ProbabilityThe conditional probability of event $A$ given $B$ is defined as:$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad \text{if } P(B) > 0$$Similarly:$$P(B \mid A) = \frac{P(A \cap B)}{P(A)}, \quad \text{if } P(A) > 0$$$P(A \cap B) =$ Probability that both $A$ and $B$ occur$P(A \mid B) =$ Probability that $A$ occurs given $B$ has occurred2. Express $P(A \cap B)$ in Two WaysFrom the conditional probability definitions:$$P(A \cap B) = P(A \mid B)P(B)$$$$P(A \cap B) = P(B \mid A)P(A)$$✅ Both are equal because $P(A \cap B)$ is symmetric: $A \cap B = B \cap A$.3. Equate the Two Expressions$$P(A \mid B)P(B) = P(B \mid A)P(A)$$4. Solve for $P(A \mid B)$$$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}$$✅ This is Bayes' Theorem (basic form).5. Express $P(B)$ Using Law of Total ProbabilityIf events $A_1, A_2, \dots, A_n$ are mutually exclusive and exhaustive (cover all possibilities), then:$$P(B) = \sum_{i=1}^{n} P(B \cap A_i) = \sum_{i=1}^{n} P(B \mid A_i)P(A_i)$$Plug this into Bayes' theorem for multiple hypotheses:$$P(A_j \mid B) = \frac{P(B \mid A_j)P(A_j)}{\sum_{i=1}^{n} P(B \mid A_i)P(A_i)}$$✅ This is the extended Bayes' theorem for multiple events.[Chart] Derivation of Bayes' TheoremConditional Probability$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \text{ if } P(B) > 0 \quad \text{if } P(A) > 0$$$P(A \cap B)$: Probability that both A and B occur$P(B \mid A)$: Probability of A given B has occurredExpress $P(A \cap B)$ in Two Ways$$P(A \cap B) = P(A \mid B) \cdot P(B)$$$$P(A \cap B) = P(B \mid A) \cdot P(A)$$Equate and Solve for $P(A \mid B)$$$P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)$$$$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}$$Using Law of Total ProbabilityMutually exclusive and exhaustive events $A_1, A_2, \dots, A_n$$$P(B) = \sum_{i=1}^{n} P(B \cap A_i) = \sum_{i=1}^{n} P(B \mid A_i) \cdot P(A_i)$$$$P(A_j \mid B) = \frac{P(B \mid A_j) \cdot P(A_j)}{P(B)}$$Using Law of Total ProbabilityMutually exclusive events $A_1, A_2, \dots, A_n$Independent EventsIf A and B are independent:$$P(A \mid B) = P(A)$$Mutually Exclusive EventsIf A and B are mutually exclusive:$$P(A \cap B) = 0 \Rightarrow P(A \mid B) = 0$$Handwritten Annotations[Venn Diagram]A Venn diagram showing two intersecting circles, $A$ and $B$, inside a rectangle.Circle A: Labeled "Cancer"Circle B: Labeled "test is +ive"Intersection ($A \cap B$): Labeled "TP" (True Positive)Left part of A (A without B): Labeled "FN" (False Negative)Right part of B (B without A): Labeled "FP" (False Positive)Outside both circles: Labeled "TN" (True Negative)[Right Side Notes]$\rightarrow$ probability of cancer given +ive testif $P(A \mid B) \Rightarrow$ [Small diagram of circle B intersecting A]if $P(B \mid A) \Rightarrow$ [Small diagram of circle A intersecting B]$\hookrightarrow$ probability of +ive test result given cancermore predictable

Bayesian Theorem Derivation, Relation to Independent Event and Mutual Exclusive Events28 January 2026 00:56Diagram Section (Top Left & Center)Bayes' Theorem$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$$P(A|B)$: Probability of A given B$P(B|A)$: Probability of B given A$P(A)$: Probability of A$P(B)$: Probability of BIndependent Events$$P(A \cap B) = P(A) \cdot P(B)$$$$P(A|B) = P(A)$$$$P(B|A) = P(B)$$Mutually Exclusive Events$$P(A \cap B) = 0$$No Overlap$$P(A|B) = 0$$No IntersectionSummary TableGeneral Bayes: $$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$Independent Events: $$P(A|B) = P(A)$$Mutually Exclusive Events: $$P(A|B) = 0$$Calculation Example Section (Top Right)Given: $P(A) = 0.3$, $P(B) = 0.4$, $P(B|A) = 0.5$General Case$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$$$P(A|B) = \frac{0.5 \cdot 0.3}{0.4} = \frac{0.15}{0.4} = 0.375$$Independent Events$$P(A|B) = P(A) = 0.3$$Mutually Exclusive Events$$P(A|B) = 0$$Text Section (Bottom Left)2. Bayes’ Theorem with Independent EventsTwo events $A$ and $B$ are independent if:$$P(A \cap B) = P(A) \cdot P(B)$$In terms of conditional probability:$$P(A \mid B) = P(A) \quad \text{and} \quad P(B \mid A) = P(B)$$Implication for Bayes’ Theorem:$$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)} = \frac{P(B)P(A)}{P(B)} = P(A)$$✅ So, if $A$ and $B$ are independent, knowing $B$ does not change the probability of $A$.Text Section (Bottom Right)3. Bayes’ Theorem with Mutually Exclusive EventsTwo events $A$ and $B$ are mutually exclusive if:$$P(A \cap B) = 0$$They cannot happen at the same time.Implication for Bayes’ Theorem:$$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}$$But since $A$ and $B$ are mutually exclusive:$$P(B \mid A) = 0 \quad \Rightarrow \quad P(A \mid B) = 0$$

Maximum A Posteriori (MAP)28 January 2026 00:19Slide Content: MAP Hypothesis and ClassificationBayes' Rule Equation:$$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$Generally, want the most probable hypothesis given the training data, Maximum a posteriori hypothesis, $h_{MAP}$:$$h_{MAP} = \operatorname*{argmax}_{h \in H} P(h|D)$$$$h_{MAP} = \operatorname*{argmax}_{h \in H} \left( \frac{P(D|h)P(h)}{P(D)} \right)$$But $P(D)$ remains the same among the items being compared, and can be eliminated:$$h_{MAP} = \operatorname*{argmax}_{h \in H} (P(D|h)P(h))$$If we assume uniform priors: $P(h_i) = P(h_j) \ \forall (i, j)$ then we can further simplify, and choose the Maximum Likelihood (ML) hypothesis:$$h_{ML} = \operatorname*{argmax}_{h_i \in H} P(D|h_i)$$Handwritten Notes & Annotations1. Diagram (Top Right)A pie chart-like diagram with sections labeled $C1$, $C2$, $C3$.2. Annotation on FormulaThere is a highlight around the argmax function with an arrow pointing to the text: "h with highest probability"3. Example (Right Side Margin)Scores:$100 \rightarrow \text{Hammad}$$67 \rightarrow \text{Hadi}$$98 \rightarrow \text{Maryam}$$70 \rightarrow \text{Fiza}$Summary:$\text{Max} = 100$$\text{Argmax} = \text{Hammad}$(Note: This handwritten example illustrates the difference between finding the maximum value versus finding the argument [the person] that produces that maximum value.)

Shy Mathematician Example28 January 2026 00:17Scenario: You run into a student who is very shy.Is he more likely to be a CS student, Business Student or Mathematics StudentBayes: Extension of the Shy MathematicianNow lets assume there are several subjects and students of various subjects vary on the graph of shynessIn a population of 600100 Mathematicians and 75% of them are shy200 Business majors and 15% of them are shy300 CS majors and 50% of them are shyNow given that a student is shy, which is the most likely subject?$P(Maths|Shy) = \frac{P(Shy|Maths) \cdot P(Maths)}{P(Shy)}$$P(Business|Shy) = \frac{P(Shy|Business) \cdot P(Business)}{P(Shy)}$$P(CS|Shy) = \frac{P(Shy|CS) \cdot P(CS)}{P(Shy)}$And we just pick the hypothesis (subject) with the highest posterior!$Subject_{MAP} = argmax_{Subject} \frac{P(Shy|Subject) \cdot P(Subject)}{P(Shy)}$Notice that $P(Shy)$ is common in the denominator and does not influence the argmax$Subject_{MAP} = argmax_{Subject} P(Shy|Subject) \cdot P(Subject)$[Handwritten & Boxed Content]$$Subject_{MAP} = argmax_{Subject} P(\text{Shy}|\text{Subject}) \cdot P(\text{Subject})$$$P(Maths|Shy) = 0.75 * \frac{100}{600} \rightarrow 0.125$$P(Business|Shy) = 0.15 * \frac{200}{600} \rightarrow 0.05$$P(CS|Shy) = 0.50 * \frac{300}{600} \rightarrow 0.25$Therefore, the student is most likely a CS major!$$h_{MAP} = Argmax \begin{pmatrix} P(Maths|Shy), \\ P(CS|Shy), \\ P(Business|Shy) \end{pmatrix}$$$$H = \{Maths, Business, CS\}$$

Classical Example of Bayes Theorem25 January 2026 23:16Suppose we want to predict whether a person buys a product (C) based on two features:$X_1 =$ Age (either "Young" or "Old")$X_2 =$ Income (either "Low" or "High")[Table Data]IDAge (X1)Income (X2)Decision (C)1YoungLowBuys2YoungLowBuys3YoungLowBuys4OldHighBuys5OldHighBuys6YoungHighBuys7YoungHighBuys8YoungHighBuys9YoungHighBuys10YoungHighBuys11YoungLowDoesn't Buy12YoungLowDoesn't Buy13OldHighDoesn't Buy14OldHighDoesn't Buy15OldHighDoesn't Buy16OldLowDoesn't Buy17OldLowDoesn't Buy18OldLowDoesn't Buy19OldLowDoesn't Buy20OldLowDoesn't BuyBayes' Theorem$$P(C \mid X_1, X_2) = \frac{P(C) \cdot P(X_1, X_2 \mid C)}{P(X_1, X_2)}$$We need to calculate both $P(C = Buys \mid X_1 = Young, X_2 = Low)$ and $P(C = Doesn't\ Buy \mid X_1 = Young, X_2 = Low)$.1. Posterior for C=Buys:Using the joint probability $P(X_1 = Young, X_2 = Low \mid C = Buys) = 0.3$ and the prior $P(C = Buys) = 0.6$:$$P(C = Buys \mid X_1 = Young, X_2 = Low) = \frac{P(C = Buys) \cdot P(X_1 = Young, X_2 = Low \mid C = Buys)}{P(X_1 = Young, X_2 = Low)}$$$$= \frac{0.5 \cdot 0.3}{P(X_1 = Young, X_2 = Low)} = \frac{0.15}{P(X_1 = Young, X_2 = Low)}$$2. Posterior for C=Doesn't:Using the joint probability $P(X_1 = Young, X_2 = Low \mid C = Doesn't\ Buy) = 0.2$ and the prior $P(C = Doesn't\ Buy) = 0.4$:$$P(C = Doesn't\ Buy \mid X_1 = Young, X_2 = Low) = \frac{P(C = Doesn't\ Buy) \cdot P(X_1 = Young, X_2 = Low \mid C = Doesn't\ Buy)}{P(X_1 = Young, X_2 = Low)}$$$$= \frac{0.5 \cdot 0.2}{P(X_1 = Young, X_2 = Low)} = \frac{0.10}{P(X_1 = Young, X_2 = Low)}$$3. Therefore,$$C_{MAP} = argmax_{c \in \{Buys, Doesn't\}} \left( \frac{0.15}{P(X_1 = Young, X_2 = Low)}, \frac{0.10}{P(X_1 = Young, X_2 = Low)} \right) = Buys$$Classification using the Bayes Theorem$$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$$$Class_{MAP} = argmax_{Class} P(Class | Features) = argmax_{Class} \frac{P(Features | Class)P(Class)}{P(Features)}$$$$Class_{MAP} = argmax_{Class} \frac{P(X_1, X_2, X_3, ..., X_n | Class)P(Class)}{P(X_1, X_2, X_3, ..., X_n)}$$$$Class_{MAP} = argmax_{Class} P(X_1, X_2, X_3, ..., X_n \mid Class) \cdot P(Class)$$But how do we calculate $P(X_1, X_2, X_3, ..., X_n \mid Class)$?This requires frequently observing all feature combinations in the dataset.Time for a Naïve Assumption: All features are conditionally independent!But they are not usually conditionally independent!That is why it's a Naïve Assumption! :)Let's solve it using Naïve Bayes step by step for:Predict C for X1 = Young, X2 = LowFrom your table (20 records):[Box 1] Step 1: Class PriorsCount each class:$Buys = 10$$Doesn't\ Buy = 10$$$P(Buys) = \frac{10}{20} = 0.5$$$$P(Doesn't\ Buy) = \frac{10}{20} = 0.5$$[Box 2] For class = BuysRecords with Buys (1–10):Age (X1)$Young = 8$$Old = 2$$$P(Young|Buys) = \frac{8}{10} = 0.8$$Income (X2)$Low = 3$$High = 7$$$P(Low|Buys) = \frac{3}{10} = 0.3$$[Box 3] For class = Doesn't BuyRecords (11–20):Age (X1)$Young = 2$$Old = 8$$$P(Young|Doesn't\ Buy) = \frac{2}{10} = 0.2$$Income (X2)$Low = 6$$High = 4$$$P(Low|Doesn't\ Buy) = \frac{6}{10} = 0.6$$[Box 4] Step 3: Apply Naive Bayes FormulaFor Buys$$P(Buys|Young, Low) \propto P(Young|Buys) \cdot P(Low|Buys) \cdot P(Buys)$$$$= 0.8 \cdot 0.3 \cdot 0.5 = 0.12$$For Doesn't Buy$$P(Doesn't\ Buy|Young, Low) \propto P(Young|Doesn't\ Buy) \cdot P(Low|Doesn't\ Buy) \cdot P(Doesn't\ Buy)$$$$= 0.2 \cdot 0.6 \cdot 0.5 = 0.06$$[Box 5] Step 4: Final Decision$$0.12 > 0.06$$✅ Predicted Class = Buys✅ Final AnswerFor X1 = Young and X2 = Low, the Naïve Bayes classifier predicts:🎯 Buys