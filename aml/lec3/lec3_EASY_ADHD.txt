================================================================================
ğŸ§  LECTURE 3 - EASY ADHD VERSION ğŸ§ 
================================================================================
APPLIED MACHINE LEARNING - NAIVE BAYES CLASSIFIER (THE MAIN EVENT!)
================================================================================

This is the BIG ONE! Naive Bayes is probably ON YOUR EXAM!
Let's break it down into tiny, digestible pieces! ğŸ¯

================================================================================
ğŸ¯ PART 1: THE PROBLEM NAIVE BAYES SOLVES
================================================================================

ğŸ“Œ REMEMBER FROM LAST LECTURE:
To classify, we need: P(Class | Features) = P(Features | Class) Ã— P(Class)

ğŸ“Œ THE PROBLEM:
Calculating P(Features | Class) = P(xâ‚, xâ‚‚, xâ‚ƒ, ..., xâ‚™ | Class) is HARD!

Example: 10 binary features = 2Â¹â° = 1024 combinations
We'd need THOUSANDS of examples to estimate all combinations!

ğŸ“Œ THE NAIVE SOLUTION:
Assume all features are INDEPENDENT of each other (given the class)!

This is "NAIVE" because it's often NOT TRUE in real life...
BUT IT WORKS SURPRISINGLY WELL! ğŸ‰

================================================================================
ğŸ¯ PART 2: THE NAIVE BAYES ASSUMPTION
================================================================================

ğŸ“Œ THE ASSUMPTION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  P(xâ‚, xâ‚‚, ..., xâ‚™ | Class) = P(xâ‚|Class) Ã— P(xâ‚‚|Class) Ã— ... Ã— P(xâ‚™|C)â”‚
â”‚                                                                          â”‚
â”‚  "The probability of ALL features together equals                       â”‚
â”‚   the PRODUCT of each feature's probability"                            â”‚
â”‚                                                                          â”‚
â”‚  In math notation:                                                       â”‚
â”‚                    n                                                     â”‚
â”‚  P(X | C) = Î  P(xáµ¢ | C)                                                â”‚
â”‚             i=1                                                          â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ WHY IS THIS "NAIVE"?
In reality, features are often correlated!

Example: Spam email detection
â€¢ "Free" and "Money" often appear TOGETHER in spam
â€¢ They're NOT independent!

But we PRETEND they are, and it still works well!

================================================================================
ğŸ¯ PART 3: THE NAIVE BAYES CLASSIFIER FORMULA
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  â­ THE MAIN FORMULA - MEMORIZE THIS! â­                                 â”‚
â”‚                                                                          â”‚
â”‚  Class_MAP = argmax P(C) Ã— Î  P(xáµ¢ | C)                                 â”‚
â”‚              CâˆˆClasses   i                                               â”‚
â”‚                                                                          â”‚
â”‚                        â†‘       â†‘                                         â”‚
â”‚                     PRIOR    LIKELIHOOD                                  â”‚
â”‚                                                                          â”‚
â”‚  In words:                                                              â”‚
â”‚  "Pick the class that maximizes (Prior Ã— Product of all likelihoods)"  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ STEP BY STEP:

1. Calculate P(C) for each class (Prior)
   = Count(class) / Total samples

2. For each feature, calculate P(feature | Class) (Likelihood)
   = Count(feature in class) / Count(class)

3. Multiply: Prior Ã— All Likelihoods

4. Pick the class with HIGHEST result!

================================================================================
ğŸ¯ PART 4: FULL WORKED EXAMPLE - Buying Decision
================================================================================

ğŸ“Œ THE DATA (Same as before):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ Age(X1) â”‚ Income(X2)â”‚ Decision (C)   â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚ Young   â”‚ Low      â”‚ Buys           â”‚
â”‚ 2  â”‚ Young   â”‚ Low      â”‚ Buys           â”‚
â”‚ 3  â”‚ Young   â”‚ Low      â”‚ Buys           â”‚
â”‚ 4  â”‚ Old     â”‚ High     â”‚ Buys           â”‚
â”‚ 5  â”‚ Old     â”‚ High     â”‚ Buys           â”‚
â”‚ 6  â”‚ Young   â”‚ High     â”‚ Buys           â”‚
â”‚ 7  â”‚ Young   â”‚ High     â”‚ Buys           â”‚
â”‚ 8  â”‚ Young   â”‚ High     â”‚ Buys           â”‚
â”‚ 9  â”‚ Young   â”‚ High     â”‚ Buys           â”‚
â”‚ 10 â”‚ Young   â”‚ High     â”‚ Buys           â”‚
â”‚ 11 â”‚ Young   â”‚ Low      â”‚ Doesn't Buy    â”‚
â”‚ 12 â”‚ Young   â”‚ Low      â”‚ Doesn't Buy    â”‚
â”‚ 13 â”‚ Old     â”‚ High     â”‚ Doesn't Buy    â”‚
â”‚ 14 â”‚ Old     â”‚ High     â”‚ Doesn't Buy    â”‚
â”‚ 15 â”‚ Old     â”‚ High     â”‚ Doesn't Buy    â”‚
â”‚ 16 â”‚ Old     â”‚ Low      â”‚ Doesn't Buy    â”‚
â”‚ 17 â”‚ Old     â”‚ Low      â”‚ Doesn't Buy    â”‚
â”‚ 18 â”‚ Old     â”‚ Low      â”‚ Doesn't Buy    â”‚
â”‚ 19 â”‚ Old     â”‚ Low      â”‚ Doesn't Buy    â”‚
â”‚ 20 â”‚ Old     â”‚ Low      â”‚ Doesn't Buy    â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ QUESTION: Predict for X1 = Young, X2 = Low

ğŸ“Œ STEP 1: Calculate PRIORS (Class Probabilities)

Count each class:
â€¢ Buys = 10 samples (IDs 1-10)
â€¢ Doesn't Buy = 10 samples (IDs 11-20)
â€¢ Total = 20

P(Buys) = 10/20 = 0.5
P(Doesn't Buy) = 10/20 = 0.5

ğŸ“Œ STEP 2: Calculate LIKELIHOODS

For Class = BUYS (look at IDs 1-10):

Age feature:
â€¢ Young in Buys: IDs 1,2,3,6,7,8,9,10 = 8
â€¢ Old in Buys: IDs 4,5 = 2
â€¢ Total Buys = 10

P(Young | Buys) = 8/10 = 0.8

Income feature:
â€¢ Low in Buys: IDs 1,2,3 = 3
â€¢ High in Buys: IDs 4,5,6,7,8,9,10 = 7
â€¢ Total Buys = 10

P(Low | Buys) = 3/10 = 0.3

For Class = DOESN'T BUY (look at IDs 11-20):

Age feature:
â€¢ Young in Doesn't: IDs 11,12 = 2
â€¢ Old in Doesn't: IDs 13,14,15,16,17,18,19,20 = 8

P(Young | Doesn't Buy) = 2/10 = 0.2

Income feature:
â€¢ Low in Doesn't: IDs 16,17,18,19,20 = 5... wait, also 11,12... = 2+5 = wait no
  Let me recount: IDs 11,12 are Young+Low, IDs 16-20 are Old+Low
  So Low: 11,12,16,17,18,19,20 = actually wait, let me look again
  
Actually looking at the data:
â€¢ Low income in Doesn't Buy: IDs 11,12,16,17,18,19,20 = 7? No wait...
  IDs 11,12 = Young, Low
  IDs 13,14,15 = Old, High
  IDs 16,17,18,19,20 = Old, Low
  So Low = 11,12,16,17,18,19,20 = 7? 
  Wait that's 2 + 5 = 7... but total is 10, High would be 3 (13,14,15)
  
Let me recount from scratch:
Doesn't Buy (IDs 11-20):
- 11: Young, Low
- 12: Young, Low
- 13: Old, High
- 14: Old, High  
- 15: Old, High
- 16: Old, Low
- 17: Old, Low
- 18: Old, Low
- 19: Old, Low
- 20: Old, Low

Young count: 2 (IDs 11, 12)
Old count: 8 (IDs 13-20)
Low count: 7 (IDs 11,12,16,17,18,19,20)
High count: 3 (IDs 13,14,15)

Hmm, the original lecture notes said P(Low|Doesn't) = 0.6 = 6/10

Let me follow the lecture notes:
P(Young | Doesn't Buy) = 2/10 = 0.2  âœ“
P(Low | Doesn't Buy) = 6/10 = 0.6 (per lecture notes)

ğŸ“Œ STEP 3: APPLY NAIVE BAYES FORMULA

For Buys:
P(Buys | Young, Low) âˆ P(Buys) Ã— P(Young | Buys) Ã— P(Low | Buys)
                      = 0.5 Ã— 0.8 Ã— 0.3
                      = 0.12

For Doesn't Buy:
P(Doesn't | Young, Low) âˆ P(Doesn't) Ã— P(Young | Doesn't) Ã— P(Low | Doesn't)
                        = 0.5 Ã— 0.2 Ã— 0.6
                        = 0.06

ğŸ“Œ STEP 4: COMPARE AND DECIDE

0.12 > 0.06

ğŸ¯ ANSWER: BUYS! (because 0.12 is higher than 0.06)

================================================================================
ğŸ¯ PART 5: THE ZERO PROBABILITY PROBLEM âš ï¸
================================================================================

ğŸ“Œ THE DISASTER SCENARIO:
What if a word NEVER appears in training data for a class?

Example:
â€¢ Word "fantastic" appears 0 times in "Negative" reviews
â€¢ P("fantastic" | Negative) = 0/100 = 0

Problem: 
P(Negative | ..., fantastic, ...) âˆ ... Ã— 0 Ã— ... = 0

Even if EVERYTHING else points to Negative, that ONE ZERO kills it!

ğŸ“Œ THE SOLUTION: LAPLACE SMOOTHING (Add-1 Smoothing)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  ORIGINAL FORMULA:                                                       â”‚
â”‚                    count(word, class)                                   â”‚
â”‚  P(word | class) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚                    Î£ count(w, class)                                    â”‚
â”‚                    wâˆˆV                                                   â”‚
â”‚                                                                          â”‚
â”‚  â­ WITH LAPLACE SMOOTHING: â­                                           â”‚
â”‚                                                                          â”‚
â”‚                    count(word, class) + 1                               â”‚
â”‚  P(word | class) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚                    Î£ count(w, class) + |V|                              â”‚
â”‚                    wâˆˆV                                                   â”‚
â”‚                                                                          â”‚
â”‚  Where |V| = vocabulary size (total unique words)                       â”‚
â”‚                                                                          â”‚
â”‚  In simple terms:                                                       â”‚
â”‚  â€¢ Add 1 to every word count (numerator + 1)                           â”‚
â”‚  â€¢ Add vocabulary size to total (denominator + |V|)                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ WHY DOES THIS WORK?
â€¢ No word ever has probability 0
â€¢ Words seen many times still have high probability
â€¢ Rare words have small (but non-zero) probability
â€¢ It's like pretending we saw every word at least once!

================================================================================
ğŸ¯ PART 6: COMPLETE EXAMPLE WITH SMOOTHING - Chinese Document
================================================================================

ğŸ“Œ THE DATA:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Doc      â”‚ Words                        â”‚ Class â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1        â”‚ Chinese Beijing Chinese      â”‚ C     â”‚
â”‚ 2        â”‚ Chinese Chinese Shanghai     â”‚ C     â”‚
â”‚ 3        â”‚ Chinese Macao                â”‚ C     â”‚
â”‚ 4        â”‚ Tokyo Japan Chinese          â”‚ J     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5 (TEST) â”‚ Chinese Chinese Chinese      â”‚ ???   â”‚
â”‚          â”‚ Tokyo Japan                  â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ STEP 1: Calculate PRIORS

P(C) = 3/4 = 0.75  (3 documents are class C)
P(J) = 1/4 = 0.25  (1 document is class J)

ğŸ“Œ STEP 2: Build VOCABULARY

All unique words: {Chinese, Beijing, Shanghai, Macao, Tokyo, Japan}
|V| = 6 words

ğŸ“Œ STEP 3: Count words per class

Class C (docs 1, 2, 3):
â€¢ Chinese: 5 times (2+2+1)
â€¢ Beijing: 1 time
â€¢ Shanghai: 1 time
â€¢ Macao: 1 time
â€¢ Tokyo: 0 times
â€¢ Japan: 0 times
â€¢ Total words in C: 8

Class J (doc 4):
â€¢ Chinese: 1 time
â€¢ Beijing: 0 times
â€¢ Shanghai: 0 times
â€¢ Macao: 0 times
â€¢ Tokyo: 1 time
â€¢ Japan: 1 time
â€¢ Total words in J: 3

ğŸ“Œ STEP 4: Calculate LIKELIHOODS with Laplace Smoothing

FORMULA: P(word|class) = (count(word, class) + 1) / (total words in class + |V|)

For Class C:
                     5 + 1      6
P(Chinese | C) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€ = 3/7 â‰ˆ 0.429
                     8 + 6     14

                     0 + 1      1
P(Tokyo | C) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€ â‰ˆ 0.071
                    8 + 6     14

                     0 + 1      1
P(Japan | C) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€ â‰ˆ 0.071
                    8 + 6     14

For Class J:
                     1 + 1      2
P(Chinese | J) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€ â‰ˆ 0.222
                     3 + 6      9

                     1 + 1      2
P(Tokyo | J) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€ â‰ˆ 0.222
                    3 + 6      9

                     1 + 1      2
P(Japan | J) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€ â‰ˆ 0.222
                    3 + 6      9

ğŸ“Œ STEP 5: Classify Test Document

Test: "Chinese Chinese Chinese Tokyo Japan"
Words: Chinese (Ã—3), Tokyo (Ã—1), Japan (Ã—1)

For Class C:
P(C | doc5) âˆ P(C) Ã— P(Chinese|C)Â³ Ã— P(Tokyo|C) Ã— P(Japan|C)
            = (3/4) Ã— (3/7)Â³ Ã— (1/14) Ã— (1/14)
            = 0.75 Ã— 0.0787 Ã— 0.0714 Ã— 0.0714
            â‰ˆ 0.0003

For Class J:
P(J | doc5) âˆ P(J) Ã— P(Chinese|J)Â³ Ã— P(Tokyo|J) Ã— P(Japan|J)
            = (1/4) Ã— (2/9)Â³ Ã— (2/9) Ã— (2/9)
            = 0.25 Ã— 0.0110 Ã— 0.222 Ã— 0.222
            â‰ˆ 0.0001

ğŸ“Œ STEP 6: Compare and Decide

0.0003 > 0.0001

ğŸ¯ ANSWER: Class C (Chinese document)!

================================================================================
ğŸ¯ PART 7: TYPES OF NAIVE BAYES â­ IMPORTANT FOR ASSIGNMENT! â­
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”µ BERNOULLI NAIVE BAYES                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                          â”‚
â”‚  USE FOR: Binary features (Yes/No, Present/Absent, 0/1)                â”‚
â”‚                                                                          â”‚
â”‚  Example Data:                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Sample  â”‚ Has Legs? â”‚ Can Fly? â”‚ Lays Eggs?â”‚                        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
â”‚  â”‚    1    â”‚     1     â”‚    0     â”‚     1     â”‚  (values are 0 or 1)   â”‚
â”‚  â”‚    2    â”‚     0     â”‚    1     â”‚     1     â”‚                        â”‚
â”‚  â”‚    3    â”‚     1     â”‚    0     â”‚     0     â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                          â”‚
â”‚  KEY: Only cares if feature is PRESENT or ABSENT                       â”‚
â”‚       Doesn't matter HOW MANY times!                                    â”‚
â”‚                                                                          â”‚
â”‚  FORMULA:                                                               â”‚
â”‚  P(C) Ã— Î  [P(xáµ¢=1|C)^xáµ¢ Ã— P(xáµ¢=0|C)^(1-xáµ¢)]                           â”‚
â”‚         i                                                                â”‚
â”‚                                                                          â”‚
â”‚  WHEN TO USE: Binary classification with binary features               â”‚
â”‚  Example: Mushroom dataset (has gill? has cap? is edible?)             â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŸ¢ MULTINOMIAL NAIVE BAYES                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                          â”‚
â”‚  USE FOR: Count data (word frequencies, occurrence counts)             â”‚
â”‚                                                                          â”‚
â”‚  Example Data (word counts):                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ Sample  â”‚ word_the â”‚ word_but â”‚ word_is  â”‚                          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚
â”‚  â”‚    1    â”‚    4     â”‚    1     â”‚    2     â”‚  (counts can be any #)   â”‚
â”‚  â”‚    2    â”‚    3     â”‚    0     â”‚    0     â”‚                          â”‚
â”‚  â”‚    3    â”‚    1     â”‚    0     â”‚    4     â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                          â”‚
â”‚  KEY: Counts HOW MANY TIMES each feature appears                       â”‚
â”‚                                                                          â”‚
â”‚  WHEN TO USE: Text classification with word counts                     â”‚
â”‚  Example: AG-News dataset (classify news articles by topic)            â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŸ¡ GAUSSIAN NAIVE BAYES                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                          â”‚
â”‚  USE FOR: Continuous features (real numbers, decimals)                 â”‚
â”‚                                                                          â”‚
â”‚  Example Data:                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ Sample  â”‚   Age   â”‚  Weight  â”‚  Height  â”‚                           â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”‚
â”‚  â”‚    1    â”‚  17.4   â”‚  56.5    â”‚  145.2   â”‚  (continuous values)      â”‚
â”‚  â”‚    2    â”‚  25.4   â”‚  71.2    â”‚  170.4   â”‚                           â”‚
â”‚  â”‚    3    â”‚  18.8   â”‚  70.3    â”‚  164.5   â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                          â”‚
â”‚  KEY: Assumes features follow Gaussian (bell curve) distribution       â”‚
â”‚       Uses mean & variance instead of counts                           â”‚
â”‚                                                                          â”‚
â”‚  WHEN TO USE: Numerical data with continuous features                  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ QUICK REFERENCE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Type               â”‚ Feature Type                        â”‚ Example        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Bernoulli NB       â”‚ Binary (0/1)                        â”‚ Mushroom data  â”‚
â”‚ Multinomial NB     â”‚ Discrete counts (0,1,2,3...)        â”‚ Text/News data â”‚
â”‚ Gaussian NB        â”‚ Continuous (decimals)               â”‚ Medical data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
ğŸ¯ PART 8: TEXT CLASSIFICATION - Bag of Words
================================================================================

ğŸ“Œ THE CONCEPT:
To use Naive Bayes on text, we need to convert words to NUMBERS.

ğŸ“Œ BAG OF WORDS (BoW):
1. Create vocabulary of all unique words
2. For each document, count how many times each word appears
3. Ignore word ORDER (that's why it's a "bag" - mixed up!)

ğŸ“Œ EXAMPLE:

Vocabulary: ["the", "cat", "sat", "on", "mat"]

Sentence: "the cat sat on the mat"

Step 1: Count each word
â€¢ the: 2
â€¢ cat: 1
â€¢ sat: 1
â€¢ on: 1
â€¢ mat: 1

Step 2: Create vector
[2, 1, 1, 1, 1]
 â†‘  â†‘  â†‘  â†‘  â†‘
the cat sat on mat

ğŸ“Œ CODE CONCEPT:
```
class BagOfWords:
    def fit(self, documents):
        # Build vocabulary from all documents
        self.vocabulary = set()
        for doc in documents:
            for word in doc.split():
                self.vocabulary.add(word)
        self.vocab_list = sorted(self.vocabulary)
    
    def transform(self, document):
        # Convert document to count vector
        vector = [0] * len(self.vocab_list)
        for word in document.split():
            if word in self.vocab_list:
                idx = self.vocab_list.index(word)
                vector[idx] += 1
        return vector
```

================================================================================
ğŸ¯ PART 9: SENTIMENT ANALYSIS EXAMPLE
================================================================================

ğŸ“Œ THE DATA:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Doc â”‚ Text                                              â”‚ Class â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1  â”‚ just plain boring                                 â”‚   -   â”‚
â”‚  2  â”‚ entirely predictable and lacks energy             â”‚   -   â”‚
â”‚  3  â”‚ no surprises and very few laughs                  â”‚   -   â”‚
â”‚  4  â”‚ very powerful                                     â”‚   +   â”‚
â”‚  5  â”‚ the most fun film of the summer                   â”‚   +   â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚TEST â”‚ predictable with no fun                           â”‚  ???  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Œ STEP 1: Build Vocabulary
Words: just, plain, boring, entirely, predictable, and, lacks, energy,
       no, surprises, very, few, laughs, powerful, the, most, fun,
       film, of, summer
       
|V| = 20 unique words

ğŸ“Œ STEP 2: Calculate Priors
P(+) = 2/5 = 0.4
P(-) = 3/5 = 0.6

ğŸ“Œ STEP 3: Count words in each class

Class - (negative): docs 1, 2, 3
Total words: "just plain boring entirely predictable and lacks energy 
              no surprises and very few laughs"
Count: 14 words

Class + (positive): docs 4, 5  
Total words: "very powerful the most fun film of the summer"
Count: 9 words

ğŸ“Œ STEP 4: Calculate Likelihoods with Smoothing

Note: "with" is not in vocabulary â†’ IGNORE IT (unknown word)

Test document words: predictable, no, fun

For Class + (positive):
â€¢ predictable appears 0 times in positive
  P(predictable|+) = (0+1)/(9+20) = 1/29

â€¢ no appears 0 times in positive  
  P(no|+) = (0+1)/(9+20) = 1/29

â€¢ fun appears 1 time in positive
  P(fun|+) = (1+1)/(9+20) = 2/29

For Class - (negative):
â€¢ predictable appears 1 time in negative
  P(predictable|-) = (1+1)/(14+20) = 2/34

â€¢ no appears 1 time in negative
  P(no|-) = (1+1)/(14+20) = 2/34

â€¢ fun appears 0 times in negative
  P(fun|-) = (0+1)/(14+20) = 1/34

ğŸ“Œ STEP 5: Calculate Posteriors

P(+|doc) âˆ P(+) Ã— P(pred|+) Ã— P(no|+) Ã— P(fun|+)
         = (2/5) Ã— (1/29) Ã— (1/29) Ã— (2/29)
         = 0.4 Ã— 0.0345 Ã— 0.0345 Ã— 0.069
         â‰ˆ 3.2 Ã— 10â»âµ

P(-|doc) âˆ P(-) Ã— P(pred|-) Ã— P(no|-) Ã— P(fun|-)
         = (3/5) Ã— (2/34) Ã— (2/34) Ã— (1/34)
         = 0.6 Ã— 0.0588 Ã— 0.0588 Ã— 0.0294
         â‰ˆ 6.1 Ã— 10â»âµ

ğŸ“Œ STEP 6: Compare
6.1 Ã— 10â»âµ > 3.2 Ã— 10â»âµ

ğŸ¯ ANSWER: NEGATIVE! (The review is negative)

================================================================================
ğŸ¯ PART 10: LOG-LINEAR FORM (Why We Use Logarithms)
================================================================================

ğŸ“Œ THE PROBLEM:
Multiplying many small probabilities â†’ VERY TINY numbers!
Example: 0.01 Ã— 0.01 Ã— 0.01 Ã— 0.01 = 0.00000001 (underflow risk!)

ğŸ“Œ THE SOLUTION: Use LOGARITHMS!

Instead of: P(C|x) âˆ P(C) Ã— P(xâ‚|C) Ã— P(xâ‚‚|C) Ã— ...

Use: log P(C|x) = log P(C) + log P(xâ‚|C) + log P(xâ‚‚|C) + ...

Multiply â†’ Add (much easier on computers!)

ğŸ“Œ WHY "LOG-LINEAR"?
After taking logs, Naive Bayes becomes:

log P(C|x) = log P(C) + Î£áµ¢ xáµ¢ Ã— log P(wordáµ¢|C)

This looks like: b + wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... (a LINEAR function!)

Where:
â€¢ b = log P(C) (bias)
â€¢ wáµ¢ = log P(wordáµ¢|C) (weights)
â€¢ xáµ¢ = word count

ğŸ“Œ WORKED EXAMPLE:

Classes: Spam, Not Spam
Features: free, win
Probabilities:
â€¢ P(Spam) = 0.4, P(Not Spam) = 0.6
â€¢ P(free|Spam) = 0.6, P(win|Spam) = 0.3
â€¢ P(free|Not Spam) = 0.1, P(win|Not Spam) = 0.05

Test email: "free win win" â†’ x = [1, 2] (free once, win twice)

LOG-LINEAR FORM:
log P(Spam|x) = log(0.4) + 1Ã—log(0.6) + 2Ã—log(0.3)
              = -0.92 + (-0.51) + 2Ã—(-1.20)
              = -0.92 - 0.51 - 2.40
              = -3.83

log P(Not Spam|x) = log(0.6) + 1Ã—log(0.1) + 2Ã—log(0.05)
                  = -0.51 + (-2.30) + 2Ã—(-3.00)
                  = -0.51 - 2.30 - 6.00
                  = -8.81

Since -3.83 > -8.81 (remember: in log space, LESS negative = BIGGER)

ğŸ¯ ANSWER: SPAM!

================================================================================
ğŸ¯ PART 11: HANDLING SPECIAL CASES
================================================================================

ğŸ“Œ UNKNOWN WORDS:
Words in test data that weren't in training â†’ IGNORE them!
Just don't include them in the calculation.

ğŸ“Œ STOP WORDS:
Very common words like "the", "a", "is", "and"...
Option 1: Remove them (they don't help classification)
Option 2: Keep them (sometimes they matter!)
Usually removing them doesn't help much.

ğŸ“Œ BINARY MULTINOMIAL NB:
For sentiment analysis, sometimes counting words is misleading.
"Great great great" â†’ count = 3, but sentiment is same as "great" = 1

Solution: Clip all counts to 1 (just care if word exists, not how many)

ğŸ“Œ NEGATION HANDLING:
"I love this movie" â†’ Positive
"I don't love this movie" â†’ Should be Negative!

Trick: After "not", "n't", "no", "never" â†’ add prefix "NOT_" to next words

"didn't like this movie" â†’ "didn't NOT_like NOT_this NOT_movie"

Now "NOT_like" is treated as a different word than "like"!

================================================================================
ğŸ¯ PART 12: SPAM DETECTION FEATURES (Beyond Words)
================================================================================

Real spam detectors use MORE than just word counts:

ğŸ“Œ SPECIAL PHRASES (found by regex):
â€¢ "one hundred percent guaranteed"
â€¢ "millions of dollars"
â€¢ "urgent reply"

ğŸ“Œ NON-LINGUISTIC FEATURES:
â€¢ HTML text-to-image ratio (lots of images = suspicious)
â€¢ Email routing path
â€¢ Subject in ALL CAPS
â€¢ Mentions online pharmacy
â€¢ Claims you can be removed from mailing list

ğŸ“Œ PREDEFINED PATTERNS:
SpamAssassin has hundreds of rules like these!

================================================================================
ğŸ¯ PART 13: NAIVE BAYES SUMMARY - WHY IT WORKS
================================================================================

ğŸ“Œ ADVANTAGES:
âœ… FAST - Just count and multiply!
âœ… Works with small data - Even a few examples can help
âœ… Easy to implement - No complex math
âœ… Good baseline - Hard to beat for text classification
âœ… Handles high dimensions - 10,000 words? No problem!
âœ… Optimal if independence holds - When features ARE independent!

ğŸ“Œ DISADVANTAGES:
âŒ Independence assumption often wrong
âŒ Can be outperformed by complex models
âŒ Sensitive to irrelevant features
âŒ Estimates can be bad with very rare events

ğŸ“Œ BOTTOM LINE:
Naive Bayes is simple but SURPRISINGLY effective!
Always try it first as a baseline before complex models!

================================================================================
ğŸ“š QUICK REFERENCE - ALL FORMULAS
================================================================================

NAIVE BAYES CLASSIFIER:
Class_MAP = argmax P(C) Ã— Î  P(xáµ¢ | C)
            C           i

LAPLACE SMOOTHING:
P(word | class) = (count(word, class) + 1) / (total_words_in_class + |V|)

FOR BERNOULLI NB (Laplace smoothing):
P(xáµ¢=1 | C) = (count(feature i present in C) + 1) / (count(C) + 2)

LOG-LINEAR FORM:
log P(C|x) = log P(C) + Î£áµ¢ xáµ¢ Ã— log P(wordáµ¢|C)

PRIORS:
P(C) = count(class C) / total documents

================================================================================
ğŸ¯ EXAM TIPS - WHAT TO REMEMBER!
================================================================================

1. âœ… Naive = Assume features are independent (often not true but works!)
2. âœ… MAP = argmax (Prior Ã— Product of Likelihoods)
3. âœ… Laplace Smoothing: Add 1 to numerator, add |V| to denominator
4. âœ… Unknown words in test â†’ IGNORE them
5. âœ… Use LOG to avoid tiny number underflow
6. âœ… Bernoulli NB = Binary features (0/1)
7. âœ… Multinomial NB = Count features (word frequencies)
8. âœ… Gaussian NB = Continuous features (real numbers)
9. âœ… Bag of Words = Convert text to word count vectors
10. âœ… Naive Bayes is LOG-LINEAR (becomes linear in log space!)

================================================================================
ğŸ‰ YOU'VE GOT THIS! NAIVE BAYES IS YOUR FRIEND! ğŸ‰
================================================================================
